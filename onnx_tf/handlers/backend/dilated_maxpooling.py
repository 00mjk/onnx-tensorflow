from __future__ import division

import tensorflow as tf
from numpy import inf


def _calc_input_ind(ind, k, d, s):
    """
        This function maps index from the output of _calc_dilated_pool
        to index from the original input inside single axis

        Args:
            ind:      vector with indices from the output pool to be mapped
            k:        tuple with kernel size along the axis
            d:        tuple with dilations
            s:        tuple with strides
        Return:
            input_ind: calculated index in the original input

        The formula is: input_ind = (ind // kH) * sH + (ind % kH) * dH

        Example:
          If we have following input to _calc_dilated_pool:
                     [[  0,  1,  2,  3],
                      [  4,  5,  6,  7],
                      [  8,  9, 10, 11],
                      [ 12, 13, 14, 15]]
          and Kernel = [2, 2], Dilations: [2, 2], Strides: [1, 1]

          the output dilated_pool shape will be [4, 4] and _calc_input_ind
          will be called twice for the two axis 0 (along height) and
          1 (along width) with

              ind = [0, 1, 2, 3]

          which will result in:

              input_ind = [0, 2, 1, 3]
    """
    return (ind // k) * (s - k * d) + ind * d


def _calc_indexes(y, x, in_width, output_height, output_width):
    """
        Calculate combined index from y and x to be used for
        gather_nd operation

        Arg:
            y:              vector with indices across the height dimension
            x:              vector with indices across the width dimension
            in_width:       width size of the input
            output_height:  height size of the output
            output_width:   width size of the output
        Return:
            out_indices:    calculated indices

        Formula is ind = y * in_width + x for every y and x

        Example input:
          y = [0, 2, 1, 3]
          x = [0, 2, 1, 3]
          in_width = 4
          output_width = 2

          1. y = [[ 0],
                  [ 8],
                  [ 4],
                  [12]]

          2. x = [[0, 2, 1, 3]]

          3a. out_indices = [[  0,  2,  1,  3],
                             [  8, 10,  9, 11],
                             [  4,  6,  5,  7],
                             [ 12, 14, 13, 15]]

          3b. out_indices = [[0, 2, 1, 3, 8, 10, 9, 11, 4, 6,
                              5, 7, 12, 14, 13, 15]]

    """

    y = tf.reshape(y * in_width, [-1, 1])
    x = tf.reshape(x, [1, -1])

    ind = tf.reshape(y + x, [output_height * output_width, 1])
    return ind


def _calc_orig_ind(ind, kH, kW, dH, dW, sH, sW, output_width, in_width, pads):
    """
        Map result indices (on top of the dilated pool) to the original input
        indices

        Maps indices generated by maxpool_with_argmax on top of the dilated
        pool to the orignal input indices
    """

    ind_shape = tf.shape(ind, out_type=tf.dtypes.int64)
    num_channels = ind_shape[3]

    # mod_floor op is not implemented on GPU
    # implement it using: a % b = a - (a // b) * b

    # inY = (ind // num_channels) // output_width
    # inX = (ind // num_channels) % output_width
    # ind_channel = ind % num_channels

    ind_ = ind // num_channels
    inY = ind_ // output_width
    inX = ind_ - (ind_ // output_width) * output_width

    ind_channel = ind - ind_ * num_channels

    y = _calc_input_ind(inY, kH, dH, sH) - pads[0]
    x = _calc_input_ind(inX, kW, dW, sW) - pads[2]

    new_ind = num_channels * (y * in_width + x) + ind_channel
    return new_ind


def _pad_input_same(input, strides, filter_height, filter_width,
                    padding="SAME_UPPER"):
    """
        Apply SAME padding to the input
    """

    input_shape = tf.shape(input, out_type=tf.dtypes.int64)
    in_height = input_shape[1]
    in_width = input_shape[2]

    out_height = tf.cast(tf.math.ceil(in_height / strides[0]), tf.int64)
    out_width = tf.cast(tf.math.ceil(in_width / strides[1]), tf.int64)

    pad_along_height = tf.math.maximum((out_height - 1) * strides[0] +
                                       filter_height - in_height, 0)
    pad_along_width = tf.math.maximum((out_width - 1) * strides[1] +
                                      filter_width - in_width, 0)
    if padding.lower() == "same_lower":
        pad_op = tf.math.ceil
    else:
        pad_op = tf.math.floor
    pad_top = tf.cast(pad_op(pad_along_height / 2), dtype=tf.int64)
    pad_bottom = pad_along_height - pad_top
    pad_left = tf.cast(pad_op(pad_along_width / 2), dtype=tf.int64)
    pad_right = pad_along_width - pad_left

    tf_paddings = [[0, 0], [pad_top, pad_bottom], [pad_left, pad_right],
                   [0, 0]]

    padded = tf.pad(input, tf_paddings, mode='CONSTANT',
                    constant_values=-inf)
    return (padded, [pad_top, pad_bottom, pad_left, pad_right])


def _pad_input_explicit(input, pads):
    """
        Apply explicit padding to the input
    """

    if pads == [0] * 4:
        return (input, [0] * 4)
    tf_paddings = [[0, 0], [pads[0], pads[2]],
                   [pads[1], pads[3]], [0, 0]]
    padded = tf.pad(input, tf_paddings, mode='CONSTANT',
                    constant_values=-inf)
    return (padded, [pads[0], pads[2], pads[1], pads[3]])


def _calc_padding_ceil_mode(input, strides, filter_height, filter_width):
    """
        Calculate padding in ceil_mode
    """

    input_shape = tf.shape(input, out_type=tf.dtypes.int64)
    in_height = input_shape[1]
    in_width = input_shape[2]

    out_h = (in_height - filter_height) / strides[0]
    pad_h = tf.math.ceil(out_h) - tf.math.floor(out_h)
    out_w = (in_width - filter_width) / strides[1]
    pad_w = tf.math.ceil(out_w) - tf.math.floor(out_w)

    pad_bottom = pad_h * strides[0]
    pad_right = pad_w * strides[1]

    paddings = [[0, 0], [0, pad_bottom], [0, pad_right], [0, 0]]

    padded = tf.pad(input, paddings, mode='CONSTANT',
                    constant_values=-inf)
    return (padded, [0, pad_bottom, 0, pad_right])


def _pad_input(input, strides, filter_height, filter_width,
               padding, ceil_mode):
    """
        Pad the input according to the parameters
    """

    pads = tf.zeros([4], tf.int64)
    # check for explicit padding
    if type(padding) is list:
        input, pads_ = _pad_input_explicit(input, padding)
        pads += pads_
    elif padding[:4].lower() == "same":
        input, pads_ = _pad_input_same(input, strides, filter_height,
                                       filter_width, padding)
        pads += pads_

    # when padding is set to SAME, ceil_mode will not do anything
    # because output sizes will be multiple of the strides
    if ceil_mode and (type(padding) is list or padding[:4].lower() != "same"):
        input, pads_ = _calc_padding_ceil_mode(input, strides,
                                               filter_height, filter_width)
        pads += pads_
    return (input, tf.cast(pads, tf.int64))


def _calc_dilated_pool(input, ksize, strides, dilation, padding, ceil_mode):
    """
        This function extracts the values from the input for every sliding
        window according to the dilations, strides and kernel size and
        generates output that can be used by max_pool2d operation with
        strides = ksize to accomplish dilated pooling

        Example:
          Input:     [[  0,  1,  2,  3],
                      [  4,  5,  6,  7],
                      [  8,  9, 10, 11],
                      [ 12, 13, 14, 15]]

          Kernel:    [2, 2]
          Dilations: [2, 2]
          Strides:   [1, 1]

          Will return:
                     [[  0,  2,  1,  3],
                      [  8, 10,  9, 11],
                      [  4,  6,  5,  7],
                      [ 12, 14, 13, 15]]

          After max_pool2d with ksize = strides = [2, 2] the result is:
                     [[ 10, 11],
                      [ 14, 15]]

          The function will also pad the input according to the paddings
          provided and if ceil_mode is enabled
    """

    kH, kW = ksize
    sH, sW = strides
    dH, dW = dilation

    # size of the input filter window calculated from the kernel and dilations
    filter_height = (kH - 1) * dH + 1
    filter_width = (kW - 1) * dW + 1

    # pad the input according to the paddings and ceil_mode
    input, pads = _pad_input(input, strides, filter_height, filter_width,
                             padding, ceil_mode)

    # NHWC to NCHW
    inputs_ = tf.transpose(input, [0, 3, 1, 2])
    input_shape = tf.shape(inputs_, out_type=tf.dtypes.int64)

    # input height and width
    in_height = input_shape[2]
    in_width = input_shape[3]

    # size of the output pool
    output_height = (((in_height - filter_height) // sH) + 1) * kH
    output_width = (((in_width - filter_width) // sW) + 1) * kW

    # for every value in the output pool, calculate the corresponding
    # indexes in the two image dimensions (y and x)
    y = tf.range(output_height)
    y = _calc_input_ind(y, kH, dH, sH)

    x = tf.range(output_width)
    x = _calc_input_ind(x, kW, dW, sW)

    # calculate the indexes for the gather_nd operation
    ind = _calc_indexes(y, x, in_width, output_height, output_width)

    # expand the Tensor to fit the batch and channel dimentions
    ind_ = tf.expand_dims(ind, 0)
    ind_ = tf.expand_dims(ind_, 0)
    ind_ = tf.tile(ind_, [input_shape[0], input_shape[1], 1, 1])

    # merge the H and W dimentions
    inputs_ = tf.reshape(inputs_, [input_shape[0], input_shape[1],
                                   input_shape[2] * input_shape[3]])

    # extract the selected values from the input
    new_pool = tf.gather_nd(inputs_, ind_, batch_dims=2)

    # reshape back to 4D
    new_pool = tf.reshape(new_pool, [input_shape[0], input_shape[1],
                                     output_height, output_width])
    # To NHWC
    new_pool = tf.transpose(new_pool, [0, 2, 3, 1])

    return (new_pool, output_width, pads)


def dilated_maxpool_with_argmax(input, ksize, strides, dilation,
                                padding="VALID", ceil_mode=False):
    """
        Do a dilated maxpool and also return indices/argmax
    """
    kH, kW = ksize
    sH, sW = strides
    dH, dW = dilation

    new_pool, output_width, pads = _calc_dilated_pool(input, ksize, strides,
                                                      dilation, padding,
                                                      ceil_mode)
    kernel = [1] + list(ksize) + [1]
    maxpool, ind = tf.nn.max_pool_with_argmax(new_pool, ksize=kernel,
                                              strides=kernel, padding="VALID")

    input_shape = tf.shape(input, out_type=tf.dtypes.int64)
    in_width = input_shape[2]
    new_ind = _calc_orig_ind(ind, kH, kW, dH, dW, sH, sW,
                             output_width, in_width, pads)

    return (maxpool, new_ind)


def dilated_maxpool2d(input, ksize, strides, dilation,
                      padding="VALID", ceil_mode=False):
    """
        Do dilated maxpool utilizing tf.nn.dilation2d.
        Pads the input if explicit padding / SAME_UPPER is provided or
        ceil_mode is True
    """
    # size of one filter window calculated from the kernel and dilation
    filter_height = (ksize[0] - 1) * dilation[0] + 1
    filter_width = (ksize[1] - 1) * dilation[1] + 1

    if type(padding) is list:
        input, _ = _pad_input_explicit(input, padding)
        padding_ = "VALID"
    elif padding.lower() == "same_lower":
        # Tensorflow does not support SAME_LOWER padding
        input, _ = _pad_input_same(input, strides, filter_height,
                                   filter_width, padding)
        padding_ = "VALID"
    elif padding.lower() == "same_upper":
        padding_ = "SAME"
    else:
        padding_ = padding

    # when padding is set to SAME, ceil_mode will not do anything
    # because output sizes will be multiple of the strides
    if ceil_mode and (type(padding) is list or padding[:4].lower() != "same"):
        input, _ = _calc_padding_ceil_mode(input, strides,
                                           filter_height, filter_width)

    strides = [1] + list(strides) + [1]
    dilation = [1] + list(dilation) + [1]

    input_shape = tf.shape(input, out_type=tf.dtypes.int64)

    filter = tf.zeros([ksize[0], ksize[1], input_shape[3]], input.dtype)

    maxpool = tf.nn.dilation2d(input=input, filter=filter, strides=strides,
                               rates=dilation, padding=padding_)

    return maxpool


def dilated_maxpool2d_v2(input, ksize, strides, dilation,
                         padding="VALID", ceil_mode=False):
    """
        Execute dilated maxpool using _calc_dilate_pool
    """

    new_pool, _, _ = _calc_dilated_pool(input, ksize, strides,
                                        dilation, padding, ceil_mode)

    kernel = [1] + list(ksize) + [1]
    maxpool = tf.nn.max_pool2d(new_pool, ksize=kernel,
                               strides=kernel, padding="VALID")
    return maxpool
